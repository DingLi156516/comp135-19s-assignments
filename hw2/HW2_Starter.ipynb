{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP 135 Spring 2019: HW2 Solution Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup comp135_env package importsÂ¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "import sklearn.preprocessing\n",
    "import sklearn.pipeline\n",
    "import sklearn.linear_model\n",
    "import sklearn.metrics\n",
    "import sklearn.model_selection\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup student-defined imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# None needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Auto dataset for Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr_MF = np.loadtxt('data_auto/x_train.csv', delimiter=',', skiprows=1)\n",
    "x_va_NF = np.loadtxt('data_auto/x_valid.csv', delimiter=',', skiprows=1)\n",
    "x_te_PF = np.loadtxt('data_auto/x_test.csv', delimiter=',', skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr_M = np.loadtxt('data_auto/y_train.csv', delimiter=',', skiprows=1)\n",
    "y_va_N = np.loadtxt('data_auto/y_valid.csv', delimiter=',', skiprows=1)\n",
    "y_te_P = np.loadtxt('data_auto/y_test.csv', delimiter=',', skiprows=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1a: Polynomial Degree Selection on Fixed Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1a(i):** Plot the number of polynomial features $G$ vs. degree $D$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_list = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "num_poly_feat_list = []\n",
    "for degree in degree_list:    \n",
    "    poly_transformer = sklearn.preprocessing.PolynomialFeatures(\n",
    "        degree=degree, include_bias=False)\n",
    "    # TODO make the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a(ii): Fit a linear regression model to a polynomial feature transformation of the provided training set of $x$, $y$ values at each of these possible degrees: [1, 2, 3, 4, 5, 6].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_list = [1, 2, 3, 4, 5, 6]\n",
    "err_tr_list = []\n",
    "err_va_list = []\n",
    "coef_list = []\n",
    "for degree in degree_list:\n",
    "    poly_transformer = sklearn.preprocessing.PolynomialFeatures(\n",
    "        degree=degree, include_bias=False)\n",
    "    # TODO fit models and make the plot\n",
    "    # Be sure to track the coefs of the learned weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1a(iii):** Based on this plot, which single degree value do you recommend? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1a(iv):** Report the numerical values of the 5th percentile and 95th percentile of the coefficients observed in your linear regression model for degrees 3, 4, 5, and 6. What seems to be happening?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1a(v):** Comment on the training error observed at degree 6. Based on your formulas from **1a(i)** and your knowledge of linear regression, what **should** the training error be at degree 6? What do you think is happening instead?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1b: Linear regression with Rescaled Polynomial Bases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup code below to make a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pipeline(degree=None):\n",
    "    pipeline = sklearn.pipeline.Pipeline(\n",
    "        steps=[\n",
    "         ('rescaler', sklearn.preprocessing.MinMaxScaler()),\n",
    "         ('poly_transformer', sklearn.preprocessing.PolynomialFeatures(degree=degree, include_bias=False)),\n",
    "         ('poly_rescaler', sklearn.preprocessing.MinMaxScaler()),\n",
    "         ('linear_regr', sklearn.linear_model.LinearRegression()),\n",
    "        ])\n",
    "    \n",
    "    # Define utility method that performs all the feature transforms of the pipeline in order.\n",
    "    def calc_features(x):\n",
    "        ''' Maps input features to rescaled polynomial features\n",
    "        '''\n",
    "        feat = x\n",
    "        for step_name, step_tfmr in pipeline.steps[:-1]:\n",
    "            feat = step_tfmr.transform(feat)\n",
    "        return feat\n",
    "    pipeline.calc_features = calc_features\n",
    "    \n",
    "    # Define utility method that accesses the linear regression weights\n",
    "    pipeline.get_linear_regr_weights = lambda: pipeline.named_steps['linear_regr'].coef_\n",
    "\n",
    "    # Return the constructed pipeline\n",
    "    # We can treat it as if it has a 'regression' API\n",
    "    # e.g. a fit and a predict method\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_list = [1, 2, 3, 4, 5, 6]\n",
    "err_tr_list = []\n",
    "err_va_list = []\n",
    "pipeline_list = []\n",
    "\n",
    "# TODO fit model at each degree and track error metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1b(i): Plot MSE for *rescaled* polynomial features vs. degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO make plot here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1b(ii):** Using this new analysis, which degree do you recommend? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1b(iii):** Report the numerical values of the 5th percentile and 95th percentile of the coefficients observed in this most recent linear regression model at all degrees. What seems to be happening? What's different than in **1a**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1c: Cross Validation setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine original train/valid splits into one mega dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trva_LF = np.vstack([x_tr_MF, x_va_NF])\n",
    "y_trva_L = np.hstack([y_tr_M, y_va_N])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Coding Step 1/1:** Complete the starter code function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mean_squared_error_across_k_folds(\n",
    "        model, x_LF, y_L, K=5, random_state=0):\n",
    "    ''' Calculate mean squared error on K cross-validation folds\n",
    "    \n",
    "    Args\n",
    "    ----\n",
    "    model: sklearn predictor object\n",
    "        has `fit` and `predict` methods\n",
    "    x_LF : 2D array, size n_examples x n_features\n",
    "        \n",
    "    y_L : 1D array, size n_examples\n",
    "    K : int\n",
    "    random_state : int or np.random.RandomState instance\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    mse_scores_K : 1D array, size K\n",
    "        estimate of MSE on each of the K folds\n",
    "    '''\n",
    "    kfold_iterator = sklearn.model_selection.KFold(\n",
    "        n_splits=K, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    # TODO compute mse on each of the K folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1c(i):** Using your `calc_mean_squared_error_across_k_folds` function with 10 folds, make a line plot of the *average* mean-squared-error at degrees 1, 2, 3, 4, 5, 6. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1c(ii):** Based on this plot, what is your recommended degree? How do your recommendations differ from *1b*?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1c(iii):** For each $K$, make a scatter plot of the $K$ fold-specific estimates of MSE ($K$ is x-axis, MSE on y-axis). Also draw a line connecting the average MSE across $K$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 3\n",
    "K_list = [2, 5, 10, 15, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140]\n",
    "pipeline = make_pipeline(degree)\n",
    "err_cv_list = []\n",
    "avg_score_list = []\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1c(iv):** What happens to the distribution of MSE as you use more and more folds? Is the trend observed in Plot **1c(iii)** what we expect based on your readings from the ISL textbook Chapter 5? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Answer here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Regularized Linear Regression via L2 and L1 penalties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Pipeline for Problem 2a: Grid Search for L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_grid = np.logspace(-9, 6, 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_L2regularized_pipeline(degree=2, alpha=1.0):\n",
    "    pipeline = sklearn.pipeline.Pipeline(\n",
    "        steps=[\n",
    "         ('rescaler', sklearn.preprocessing.MinMaxScaler()),\n",
    "         ('poly_transformer', sklearn.preprocessing.PolynomialFeatures(degree=degree, include_bias=False)),\n",
    "         ('poly_rescaler', sklearn.preprocessing.MinMaxScaler()),\n",
    "         ('linear_regr', sklearn.linear_model.Ridge(alpha=alpha)),\n",
    "        ])\n",
    "    \n",
    "    # Define utility method that performs all the feature transforms of the pipeline in order.\n",
    "    def calc_features(x):\n",
    "        tmp = x\n",
    "        for step_name, step_tfmr in pipeline.steps[:-1]:\n",
    "            tmp = step_tfmr.transform(tmp)\n",
    "        return tmp\n",
    "    pipeline.calc_features = calc_features\n",
    "    \n",
    "    pipeline.get_linear_regr_weights = lambda: pipeline.named_steps['linear_regr'].coef_\n",
    "    pipeline.get_linear_regr_bias = lambda: pipeline.named_steps['linear_regr'].intercept_\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2a: Grid Search for L2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2a(i):** Train `Ridge` regression across grid of possible L2-penalty strengths $\\alpha$. Using *degree 2* polynomial features, plot the MSE vs. regularization strength on train and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO plotting code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2a(ii):** Make the same plot at degree 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO plotting code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a(iii): How does the recommended $\\alpha$ change from deg. 2 to 6?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a(iv): Plot nonzero entries vs $\\alpha$ for degree 6?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CODE HERE"
   ]
  },
 

  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2b: Grid search for L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_L1regularized_pipeline(degree=2, alpha=1.0):\n",
    "    pipeline = sklearn.pipeline.Pipeline(\n",
    "        steps=[\n",
    "         ('rescaler', sklearn.preprocessing.MinMaxScaler()),\n",
    "         ('poly_transformer', sklearn.preprocessing.PolynomialFeatures(degree=degree, include_bias=False)),\n",
    "         ('poly_rescaler', sklearn.preprocessing.MinMaxScaler()),\n",
    "         ('linear_regr', sklearn.linear_model.Lasso(alpha=alpha, max_iter=50000)),\n",
    "        ])\n",
    "    \n",
    "    # Define utility method that performs all the feature transforms of the pipeline in order.\n",
    "    def calc_features(x):\n",
    "        tmp = x\n",
    "        for step_name, step_tfmr in pipeline.steps[:-1]:\n",
    "            tmp = step_tfmr.transform(tmp)\n",
    "        return tmp\n",
    "    pipeline.calc_features = calc_features\n",
    "    \n",
    "    pipeline.get_linear_regr_weights = lambda: pipeline.named_steps['linear_regr'].coef_\n",
    "    pipeline.get_linear_regr_bias = lambda: pipeline.named_steps['linear_regr'].intercept_\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha  1.000e-04 | NOT converged | completed 50000 iters | gap    548.999\n"
     ]
    }
   ],
   "source": [
    "alpha_grid = np.logspace(-9, 6, 31)\n",
    "degree = 3\n",
    "err_tr_list = []\n",
    "err_va_list = []\n",
    "pipeline_list = []\n",
    "for alpha in [0.0001]: # TODO loop over all alpha_grid\n",
    "    \n",
    "    # Create pipeline with specified alpha\n",
    "    pipeline = make_L1regularized_pipeline(alpha=alpha, degree=degree)\n",
    "\n",
    "    # L1 solvers are a bit more finicky than L2 solvers\n",
    "    # Here's some code that will show if each optimization has converged\n",
    "    with warnings.catch_warnings(record=True) as warn_list:\n",
    "        pipeline.fit(x_tr_MF, y_tr_M)\n",
    "        solver = pipeline.named_steps['linear_regr']\n",
    "        print(\"alpha % .3e | %s | completed %5d iters | gap % 10.3f\" % (\n",
    "            alpha, \n",
    "            'converged    ' if len(warn_list) == 0 else 'NOT converged',\n",
    "            solver.n_iter_, solver.dual_gap_))\n",
    "\n",
    "    # TODO evaluation code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b(i): Plot MSE vs. $\\alpha$ for L1 (\"Lasso\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO make plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b(ii): Plot number of non-zero coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO make plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b(iii): Print out non-zero coefficients of learned L1-regularized models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO write code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a: Adding fake observations to a no-bias linear regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** \n",
    "You are given a training dataset for regression $\\{x_n, y_n \\}_{n=1}^N$. Each feature vector $x_n$ has size $F=4$. Each $y_n$ is a scalar.\n",
    "\n",
    "You decide to add in the following additional \"fake\" observations $\\tilde{x}$:\n",
    "\n",
    "\\begin{align}\n",
    "\\begin{array}{c c c c}\n",
    "    ~\\sqrt{\\lambda}  & 0  & 0 & 0\n",
    " \\\\ 0  & \\sqrt{\\lambda}  & 0  & 0\n",
    " \\\\ 0  &0 & \\sqrt{\\lambda} &0\n",
    " \\\\ 0  & 0  & 0  &\\sqrt{\\lambda}\n",
    "\\end{array}\n",
    "\\end{align}\n",
    "\n",
    "You also add in 4 \"fake\" responses $\\tilde{y}$, all equal to zero.\n",
    "\n",
    "You then wish to fit a least squares linear regression model with no bias/intercept term to the combined labeled dataset that is the union of $x$ and $\\tilde{x}$, $y$ and $\\tilde{y}$.\n",
    "\n",
    "### **3a(i):** Write down the optimization objective and simplify as much as possible.\n",
    "\n",
    "### **3a(ii):** Does this \"add fake data\" process lead to an objectie that looks familiar?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**:\n",
    "\n",
    "TODO write using Markdown / LaTeX syntax.\n",
    "\n",
    "Or paste in an image of a handwritten solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
